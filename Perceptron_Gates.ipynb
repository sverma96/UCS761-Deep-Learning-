{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Lab Tasks (What You Must Do)**\n",
        "1) Implement a perceptron from scratch (no ML libraries).\n",
        "2) Train it separately for:\n",
        "\n",
        "*   AND\n",
        "*   OR\n",
        "\n",
        "*   NAND\n",
        "*   NOR\n",
        "\n",
        "\n",
        "*   XOR\n",
        "\n",
        "3) Show:\n",
        "\n",
        "*   final weights\n",
        "*   final bias\n",
        "\n",
        "\n",
        "\n",
        "4) Verify correct predictions for all inputs.\n",
        "\n",
        "5)Briefly explain:\n",
        "\n",
        "*   effect of learning rate\n",
        "*   why the same code learned different gates\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8Vmj1RIrPJnH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OFI3bmcDYXh"
      },
      "outputs": [],
      "source": [
        "#Perceptron Code (Single Neuron)\n",
        "class Perceptron:\n",
        "    def __init__(self, lr=0.1, epochs=20):\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.w1 = 0.0\n",
        "        self.w2 = 0.0\n",
        "        self.b = 0.0\n",
        "\n",
        "    def predict(self, x1, x2):\n",
        "        z = self.w1 * x1 + self.w2 * x2 + self.b\n",
        "        return 1 if z >= 0 else 0\n",
        "\n",
        "    def train(self, data):\n",
        "        for _ in range(self.epochs):\n",
        "            for x1, x2, y in data:\n",
        "                y_hat = self.predict(x1, x2)\n",
        "                error = y - y_hat\n",
        "\n",
        "                # Update rule\n",
        "                self.w1 += self.lr * error * x1\n",
        "                self.w2 += self.lr * error * x2\n",
        "                self.b  += self.lr * error"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Datasets for Each Logic Gate\n",
        "AND  = [(0,0,0), (0,1,0), (1,0,0), (1,1,1)]\n",
        "OR   = [(0,0,0), (0,1,1), (1,0,1), (1,1,1)]\n",
        "NAND = [(0,0,1), (0,1,1), (1,0,1), (1,1,0)]\n",
        "NOR  = [(0,0,1), (0,1,0), (1,0,0), (1,1,0)]\n",
        "XOR  = [(0,0,0), (0,1,1), (1,0,1), (1,1,0)]"
      ],
      "metadata": {
        "id": "GLc1XrhcDo72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training & Testing Function\n",
        "def run_gate(name, dataset):\n",
        "    print(f\"\\n{name} Gate\")\n",
        "    p = Perceptron(lr=0.1, epochs=20)\n",
        "    p.train(dataset)\n",
        "\n",
        "    print(f\"Weights: w1={p.w1:.2f}, w2={p.w2:.2f}\")\n",
        "    print(f\"Bias: b={p.b:.2f}\")\n",
        "\n",
        "    for x1, x2, y in dataset:\n",
        "        pred = p.predict(x1, x2)\n",
        "        print(f\"Input ({x1},{x2}) → Predicted: {pred}, Expected: {y}\")"
      ],
      "metadata": {
        "id": "R13Q0imoDuCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Run All Gates\n",
        "run_gate(\"AND\", AND)\n",
        "run_gate(\"OR\", OR)\n",
        "run_gate(\"NAND\", NAND)\n",
        "run_gate(\"NOR\", NOR)\n",
        "run_gate(\"XOR\", XOR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaHwWfLzEEul",
        "outputId": "383ee165-9df8-41a2-828a-61e79be2df9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "AND Gate\n",
            "Weights: w1=0.20, w2=0.10\n",
            "Bias: b=-0.20\n",
            "Input (0,0) → Predicted: 0, Expected: 0\n",
            "Input (0,1) → Predicted: 0, Expected: 0\n",
            "Input (1,0) → Predicted: 0, Expected: 0\n",
            "Input (1,1) → Predicted: 1, Expected: 1\n",
            "\n",
            "OR Gate\n",
            "Weights: w1=0.10, w2=0.10\n",
            "Bias: b=-0.10\n",
            "Input (0,0) → Predicted: 0, Expected: 0\n",
            "Input (0,1) → Predicted: 1, Expected: 1\n",
            "Input (1,0) → Predicted: 1, Expected: 1\n",
            "Input (1,1) → Predicted: 1, Expected: 1\n",
            "\n",
            "NAND Gate\n",
            "Weights: w1=-0.20, w2=-0.10\n",
            "Bias: b=0.20\n",
            "Input (0,0) → Predicted: 1, Expected: 1\n",
            "Input (0,1) → Predicted: 1, Expected: 1\n",
            "Input (1,0) → Predicted: 1, Expected: 1\n",
            "Input (1,1) → Predicted: 0, Expected: 0\n",
            "\n",
            "NOR Gate\n",
            "Weights: w1=-0.10, w2=-0.10\n",
            "Bias: b=0.00\n",
            "Input (0,0) → Predicted: 1, Expected: 1\n",
            "Input (0,1) → Predicted: 0, Expected: 0\n",
            "Input (1,0) → Predicted: 0, Expected: 0\n",
            "Input (1,1) → Predicted: 0, Expected: 0\n",
            "\n",
            "XOR Gate\n",
            "Weights: w1=-0.10, w2=0.00\n",
            "Bias: b=0.00\n",
            "Input (0,0) → Predicted: 1, Expected: 0\n",
            "Input (0,1) → Predicted: 1, Expected: 1\n",
            "Input (1,0) → Predicted: 0, Expected: 1\n",
            "Input (1,1) → Predicted: 0, Expected: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations**\n",
        "**1. Effect of Learning Rate (η)**\n",
        "\n",
        "Small η → slow learning\n",
        "\n",
        "Large η → oscillations\n",
        "\n",
        "η = 0.1 provides stable convergence for binary inputs"
      ],
      "metadata": {
        "id": "L6oHlk7zQz-m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Why Same Code Learns Different Gates**\n",
        "\n",
        "Architecture remains unchanged\n",
        "\n",
        "Learning rule remains unchanged\n",
        "\n",
        "Only dataset changes\n",
        "\n",
        "➡ Behavior is defined by data, not code"
      ],
      "metadata": {
        "id": "eibW4a8eQ13b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Why XOR Cannot Be Learned**\n",
        "\n",
        "XOR is not linearly separable\n",
        "\n",
        "A single perceptron creates only one linear decision boundary\n",
        "\n",
        "No straight line can separate XOR classes\n",
        "\n",
        "➡ This proves the limitation of a single perceptron"
      ],
      "metadata": {
        "id": "OtVXZ36QRAi_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion**\n",
        "\n",
        "\n",
        "\n",
        "*   A single perceptron can learn linearly separable problems\n",
        "*   AND, OR, NAND, NOR are successfully learned\n",
        "\n",
        "*   XOR fails, demonstrating the need for multi-layer networks\n",
        "\n",
        "*  Learning occurs only through weight and bias updates\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DJT2rgzBRJqX"
      }
    }
  ]
}